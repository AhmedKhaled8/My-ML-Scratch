{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is an algorithm that is very useful in classification problems. Classification problems are those in which the target $y$ is a categorical variable (nominal / ordinal). The algorithms used in these problems are usually called \"classifiers\", while in problems where the target $y$ is a continuous variable, they are called \"regressors\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "* [Introduction to Logistic Regression](#introduction-to-logistic-regression)\n",
    "* [Likelihood](#likelihood)\n",
    "* [Gradient Ascent](#gradient-ascent)\n",
    "* [Code](#code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tried to use the linear regression in a binary (2-classes) classification problem (e.g., is a tumor maliginant or not based on its size ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear Regression in Classification - case 1](assets/images/linear-regression-in-classification-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can fit this line and say when $h_\\theta(x) \\geq 0.5$,  say it's malignant and not if otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/linear-regression-in-classification-1-details.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very logical until now. If we introduced another sample,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/linear-regression-in-classification-2.png\" width=\"1500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted line is different and skewed towards the new sample and introudced a new tumor size threshold for $h_\\theta(x) \\geq 0.5$, although it should be the same as the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/linear-regression-in-classification-2-details.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the linear regression model isn't the best option for the classification problems that should find the optimal decision boundary given some data samples. And here comes the logisitic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logisitc regression model uses a hypothesis called ***Sigmoid (Logistic) function***. Which gives a value in the range of [0, 1]\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\boxed{ h_\\theta(x) = \\sigma(\\theta^TX) = \\frac{1}{1+e^{-\\theta^TX}} } \\tag{1}$$\n",
    "\n",
    "$$h_\\theta(x) \\in (0, 1)$$\n",
    "\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/sigmoid-function.png\" width=\"1000\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is used to predict the probability of $y = 1$ given $x$ which is parameterized by $\\theta$ for binary classification\n",
    "\n",
    "$$P(y = 1|x;\\theta) = h_\\theta(x)$$\n",
    "$$P(y = 0|x;\\theta) = 1 - h_\\theta(x)$$\n",
    "$$P(y|x;\\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{(1-y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the parameters $\\theta$, $L(\\theta)$, in learning algorithms is the probability of getting the true values given input if the parameters are set to specific values. The higher likelihood, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\theta) = \\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^{m}  \\left[ h_\\theta(x^{(i)})^{y^{(i)}} \\left( 1 - h_\\theta(x^{(i)}) \\right)^{(1-y^{(i)})} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the log-likelihood, $l(\\theta)$,\n",
    "\n",
    "$$\\boxed{ l(\\theta) = log(L(\\theta)) = \\sum_{i=1}^{m} \\left[ y^{(i)} h_\\theta(x^{(i)}) * (1 - y^{(i)}) \\left(1 - h_\\theta(x^{(i)})\\right) \\right] } \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the cost function is the same as **maximizing the likelihood**. We will use **gradient *ascent*** to find the parameters that maximize the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you can use the negative of the log-likelihood as the *cost* function and use gradient descent. It's just some mathematical manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the same as gradient descent. But at gradient descent, we want to find the parameters that are located at the minima of a cost function. For gradient ascent, we want to find the parameters that are located at the maxima of the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/images/gradient-descent-ascent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_j = \\theta_j + \\alpha * \\frac{\\partial{}}{\\partial{\\theta_j}}l(\\theta)$$\n",
    "\n",
    "$$\\boxed{ \\frac{\\partial{}}{\\partial{\\theta_j}}l(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} - h_\\theta \\left( x^{(i)} \\right) \\right) x_j^{(i)} } \\tag{3}$$\n",
    "\n",
    "$$\\boxed { \\theta_j := \\theta_j + \\alpha * \\frac{\\partial{}}{\\partial{\\theta_j}} l(\\theta) } \\tag{4}$$\n",
    "\n",
    "Check the logistic regression part of the [lecture notes](http://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf) of CS229 for Prof. Andrew Ng for the proof of the gradient of the log-likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression logic is the same as the linear regression. The main difference is the hypothesis, the cost (likelihood) function. Calculating the gradients and updating them are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "z^{(1)} \\\\\n",
    "z^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "z^{(m)}\n",
    "\\end{bmatrix}\n",
    "_{m * 1}\n",
    "\n",
    " =\n",
    " \n",
    "  \n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\cdots & x_n^{(3)} \\\\\n",
    "x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(4)} \n",
    "\\end{bmatrix}\n",
    "_{m * n + 1}\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "_{n + 1 * 1}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\boxed{ Z_{m * 1} = X_{m*n+1} \\hspace{1mm} \\theta_{n+1*1} } \\tag{5}$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h^{(1)} \\\\\n",
    "h^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "h^{(m)}\n",
    "\\end{bmatrix}\n",
    "_{m * 1}\n",
    "\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\sigma\\left(z^{(1)}\\right) \\\\\n",
    "\\sigma\\left(z^{(2)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma\\left(z^{(m)}\\right)\n",
    "\\end{bmatrix}\n",
    "_{m * 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{ H_{m * 1} = \\sigma \\left( Z_{m*1} \\right) } \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "l(\\theta)\n",
    "\n",
    "=\n",
    "\n",
    "\\frac{1}{m}\n",
    "\n",
    "\\left(\n",
    "\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}^T\n",
    "_{1 * m}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "log \\left( h^{(1)} \\right) \\\\\n",
    "log \\left( h^{(2)} \\right) \\\\\n",
    "\\vdots \\\\\n",
    "log \\left( h^{(m)} \\right)\n",
    "\\end{bmatrix}\n",
    "_{m * 1}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\left( 1 - y^{(1)} \\right) \\\\\n",
    "\\left( 1 - y^{(1)} \\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\left( 1 - y^{(m)} \\right)\n",
    "\\end{bmatrix}^T\n",
    "_{1 * m}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "log \\left( 1 - h^{(1)} \\right) \\\\\n",
    "log \\left( 1 - h^{(2)} \\right)  \\\\\n",
    "\\vdots \\\\\n",
    "log \\left( 1 - h^{(m)} \\right)\n",
    "\\end{bmatrix}\n",
    "_{m * 1}\n",
    "\n",
    "\n",
    "\\right)\n",
    "\n",
    "$$\n",
    "\n",
    "$$\\boxed{ l(\\theta) = \\frac{1}{m} \\left( \\hspace{1mm} Y^T_{1*m} * \\hspace{1mm} log(H)_{m*1} + \\hspace{1mm} (\\vec{1}_{m*1} - Y)^T_{1*m} * \\hspace{1mm} log(\\vec{1}_{m*1} - H)_{m*1}\\right) } \\tag{7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{l(\\theta)}}{\\partial{\\theta_0}} \\\\ \\\\\n",
    "\\frac{\\partial{l(\\theta)}}{\\partial{\\theta_1}}\\\\ \\\\\n",
    "\\vdots \\\\\\\\\n",
    "\\frac{\\partial{l(\\theta)}}{\\partial{\\theta_n}}\n",
    "\\end{bmatrix}\n",
    "_{n + 1 * 1}\n",
    "\n",
    "= \n",
    "\\frac{1}{m}\n",
    "\n",
    "\n",
    "\n",
    "\\left(\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_0^{(1)} & x_0^{(2)} & \\cdots & x_0^{(m)} \\\\\n",
    "x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)} \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix}  \n",
    "_{n+1 * m}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} - h_\\theta(x^{(1)}) \\\\\n",
    "y^{(2)} - h_\\theta(x^{(2)})\\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)} - h_\\theta(x^{(m)})\n",
    "\\end{bmatrix}\n",
    "_{m * 1}  \n",
    "\n",
    "\\right)\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$ \\boxed{ \\frac{\\partial{l(\\theta)}}{\\partial{\\theta}}_{n+1 * 1} = \\frac{1}{m} X^T_{m * n+1} (Y - H)_{m * 1} } \\tag{8}$$\n",
    "\n",
    "$$ \\boxed{\\theta_{n + 1 * 1} := \\theta_{n + 1 * 1} + \\alpha * \\frac{\\partial{l(\\theta)}}{\\partial{\\theta}} _{n + 1 * 1}} \\tag{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1 marks</th>\n",
       "      <th>Exam 2 marks</th>\n",
       "      <th>Admission status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Exam 1 marks  Exam 2 marks  Admission status\n",
       "0     34.623660     78.024693                 0\n",
       "1     30.286711     43.894998                 0\n",
       "2     35.847409     72.902198                 0\n",
       "3     60.182599     86.308552                 1\n",
       "4     79.032736     75.344376                 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"assets/data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_intercept = np.hstack((np.ones((m_train, 1)), X_train_scaled))\n",
    "X_test_intercept = np.hstack((np.ones((m_test, 1)), X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train_intercept.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.random((n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    z = X_train_intercept @ theta\n",
    "    h = sigmoid(z)\n",
    "    grads = (1 / m_train) * (X_train_intercept.T @ (h - y_train))\n",
    "    theta = theta - 0.01 * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [0, 8]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "h_hat = sigmoid(X_test_intercept @ theta)\n",
    "h_hat[h_hat >= 0.5] = 1\n",
    "h_hat[h_hat < 0.5] = 0\n",
    "confusion_matrix(y_test, h_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\U\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5,  0],\n",
       "       [ 0, 10]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'none')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr.coef_, lr.intercept_\n",
    "confusion_matrix(lr.predict(X_test_scaled), h_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.18518579],\n",
       "       [4.78832816],\n",
       "       [3.95238248]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BinaryLogisticRegression import BinaryLogisticRegression\n",
    "\n",
    "blr = BinaryLogisticRegression(X_train_scaled, y_train)\n",
    "blr.fit(n_iterations = 100000)\n",
    "blr.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = blr.predict(X_test_scaled)\n",
    "blr.score(y_test, y_pred, probabilistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Using softmax for multiclass regression."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa5216fee35f452e5fdd4427fb34069b0db31c3292a7466921a974e4c2813772"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
